import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

# ----------------------------------------------------
# 1ï¸âƒ£ Ortam deÄŸiÅŸkenlerini yÃ¼kle
# ----------------------------------------------------
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("âŒ GEMINI_API_KEY not found. Please add it to your .env file.")
    st.stop()

# ----------------------------------------------------
# 2ï¸âƒ£ BaÅŸlÄ±k
# ----------------------------------------------------
st.set_page_config(page_title="ğŸ—£ï¸ Ã‡eviri Chatbotu", layout="wide")
st.title("ğŸ—£ï¸ TÃ¼rkÃ§e â†” Ä°ngilizce Ã‡eviri Chatbotu (RAG Destekli)")
st.markdown("CÃ¼mleyi TÃ¼rkÃ§e yazarsanÄ±z Ä°ngilizceye, Ä°ngilizce yazarsanÄ±z TÃ¼rkÃ§eye Ã§evirir. Ã‡eviri Ã¶rnekleri iÃ§in RAG veritabanÄ±nÄ± kullanÄ±r.")

# ----------------------------------------------------
# 3ï¸âƒ£ Chroma veritabanÄ±nÄ± yÃ¼kle
# ----------------------------------------------------
PERSIST_DIRECTORY = "./chroma_db_translation"

if not os.path.exists(PERSIST_DIRECTORY):
    st.warning("âš ï¸ VektÃ¶r veritabanÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce `python rag_setup.py` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
    st.stop()

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004",
    client_options={"api_key": GEMINI_API_KEY}
)

try:
    db = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings
    )
    retriever = db.as_retriever(search_kwargs={"k": 5}) # Daha fazla baÄŸlam iÃ§in k=5 yapÄ±ldÄ±
except Exception as e:
    st.error(f"âŒ VeritabanÄ± yÃ¼klenirken hata oluÅŸtu: {e}")
    st.stop()

# ----------------------------------------------------
# 4ï¸âƒ£ LLM (Gemini) - RAG iÃ§in
# ----------------------------------------------------
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.3,
    client_options={"api_key": GEMINI_API_KEY}
)

# ----------------------------------------------------
# 5ï¸âƒ£ TÃ¼rkÃ§e sorgularÄ± Ä°ngilizceye Ã§eviren mini LLM (ESKÄ° MANTIK)
# ----------------------------------------------------
translator = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.2,
    client_options={"api_key": GEMINI_API_KEY}
)

def translate_to_english(text):
    """KullanÄ±cÄ±nÄ±n TÃ¼rkÃ§e metnini Ä°ngilizceye Ã§evirir (Retrieval optimizasyonu iÃ§in)."""
    # ğŸ’¡ NOT: Bu fonksiyon, LangChain'den baÄŸÄ±msÄ±z olarak doÄŸrudan LLM Ã§aÄŸrÄ±sÄ± yapar.
    try:
        # Sorgunun TÃ¼rkÃ§e olup olmadÄ±ÄŸÄ±nÄ± basitÃ§e kontrol et
        if any(char in text for char in 'Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÃ–ÅÃœ'):
             prompt = f"Translate this Turkish text into English only: {text}"
             # ESKÄ° KODDAKÄ° GÄ°BÄ° SADECE Ä°NGÄ°LÄ°ZCE Ã‡EVÄ°RÄ° Ä°STÄ°YORUZ
             return translator.invoke(prompt).content.strip()
        
        return text  # Zaten Ä°ngilizce ise, olduÄŸu gibi bÄ±rak
    except Exception as e:
        return text  # Hata olursa, orijinal sorguyu RAG zincirine gÃ¶nder
# ----------------------------------------------------
# 6ï¸âƒ£ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å PROMPT TEMPLATE (Ã‡Ä±ktÄ± hatasÄ±nÄ± Ã¶nlemek iÃ§in bu kÄ±sÄ±m temiz tutuldu)
# ----------------------------------------------------
template_text = """
You are a highly efficient bilingual translation assistant.
Your task is to provide the best possible translation for the user's QUESTION.

If the QUESTION is in Turkish, translate it into English.
If the QUESTION is in English, translate it into Turkish.

Use the provided CONTEXT (translation pairs) only as a reference or example.
If the provided CONTEXT does not contain a direct translation, use your general language knowledge to translate the QUESTION.

CONTEXT (Reference Pairs):
{context}

QUESTION: {question}

TRANSLATION (Provide ONLY the translated sentence):
"""

custom_prompt = PromptTemplate.from_template(template_text)

# ----------------------------------------------------
# 7ï¸âƒ£ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å RAG zinciri (Retrieval ve Generation adÄ±mÄ±nÄ± ayÄ±rmak iÃ§in basitleÅŸtirildi)
# ----------------------------------------------------
# Context'i alacak ve soruyu Prompt'a yerleÅŸtirecek Runnable grubu.
# ArtÄ±k soru RunnablePassthrough() deÄŸil, Ã§Ã¼nkÃ¼ bunu manuel olarak 9. adÄ±mda saÄŸlayacaÄŸÄ±z.
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG zincirinin sadece Generation (Ãœretim) kÄ±smÄ±
rag_generation_chain = (
    custom_prompt
    | llm
    | StrOutputParser()
)

# ----------------------------------------------------
# 8ï¸âƒ£ Sohbet geÃ§miÅŸi
# ----------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ----------------------------------------------------
# 9ï¸âƒ£ KullanÄ±cÄ±dan giriÅŸ al (Ã‡Ä°FT SORGULU DÃœZELTME)
# ----------------------------------------------------
user_input = st.chat_input("Ã‡eviri iÃ§in bir cÃ¼mle yazÄ±n (Ã–rn: 'GÃ¼naydÄ±n' veya 'I'm hungry')")

if user_input:
    # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("Ã‡evriliyor..."):
            try:
                # 1. Mini LLM ile Retrieval iÃ§in sorguyu Ä°ngilizceye Ã§eviriyoruz.
                retrieval_query = translate_to_english(user_input)
                
                # 2. Retrieval AdÄ±mÄ±: Ã‡evrilmiÅŸ sorguyu kullanarak veritabanÄ±ndan baÄŸlam Ã§ek.
                retrieved_docs = retriever.invoke(retrieval_query)
                context = format_docs(retrieved_docs)
                
                # 3. Generation AdÄ±mÄ±: Ana LLM'e (rag_generation_chain) *hem baÄŸlamÄ±* hem de *orijinal kullanÄ±cÄ± girdisini* gÃ¶nder.
                response = rag_generation_chain.invoke({
                    "context": context,
                    # ğŸ’¡ DÃœZELTME BURADA: Ana LLM'e Ã§evrilmiÅŸ sorguyu deÄŸil, Orijinal sorguyu gÃ¶nderiyoruz.
                    "question": user_input 
                }) 
                
                st.markdown(response)
            except Exception as e:
                response = f"âš ï¸ Hata: Ã‡eviri yapÄ±lamadÄ±. Detay: {e}"
                st.error(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
